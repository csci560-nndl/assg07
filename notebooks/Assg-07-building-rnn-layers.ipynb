{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 07: Building Recurrent NN Layers by Hand\n",
    "---\n",
    "\n",
    "**Due Date:** Tuesday 06/25/2025 (by midnight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill these in before submitting, just in case I accidentally mix up file names while grading**:\n",
    "\n",
    "Name: Jane Hacker\n",
    "\n",
    "CWID-5: (Last 5 digits of cwid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Welcome to our first assignment over Text/Sequence deep learning systems.  In this assignment\n",
    "you will implement by hand the key components of Recurrent Neural layers in NumPy.\n",
    "\n",
    "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a unidirectional RNN to take information from the past to process later inputs. A bidirectional RNN can take context from both the past and the future. \n",
    "\n",
    "**Notation**:\n",
    "\n",
    "In this notebook we will use\n",
    "\n",
    "- A superscript (in angle brackets) like $x^{\\langle t \\rangle}$ denotes some vale at the $t^{th}$ time step.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- As with the previous assignment, you will need to create the function declarations asked for\n",
    "  in `src/assg_tasks.py`.  Make sure you use\n",
    "  [Python Docstrings](https://www.geeksforgeeks.org/python-docstrings/) and are generally\n",
    "  following [Pep8 Python Style Guide](https://peps.python.org/pep-0008/) for your code.\n",
    "- Cells with `### TESTED` comment contain unit tests that are run on your implementation.  You will\n",
    "  need to uncomment the call to the unit tests, but otherwise need to stay as given in the original\n",
    "  notebook.\n",
    "- Likewise since you need to write your declaration of the functions asked for the tasks, don't forget\n",
    "  to uncomment/add the appropriate `from assg_src include X` statements in both this notebook and\n",
    "  in the `../src/test_assg_tasks.py`\n",
    "\n",
    "**In this assignment, you will:**\n",
    "\n",
    "- Implement the basic building blocks of a recurrent NN layer implementation.\n",
    "- Learn more about the modifications of an LSTM, and adding its residual connections\n",
    "  to avoid vanishing gradients issues.\n",
    "- Learn in detail the operations of recurrent layer cells and how they work.\n",
    "- See some examples of how recurrent layer operations can be unrolled in order to calculate\n",
    "  gradients over the tensor operations performed by them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "The following imports should be all of the backages that you will need for this assignment.\n",
    "We are using the Keras API in this assignment and in future assignments, so the `tensorflow` and `keras` modules\n",
    "you need are now available in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment wide imports go here, usually all of your imports for noteboosk should\n",
    "# be put up at the top here, if they were not given to you at the start of the assignment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following ipython magic will reload changed file/modules.\n",
    "# So when editing function in source code modules, you should\n",
    "# be able to just rerun the cell, not restart the whole kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports of the function you will write have been commented out here this time.  You will need to uncomment\n",
    "the imports once you declare and write your functions here, and also in the `src/test_assg_tasks.py` file to\n",
    "run the unit tests on your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions/moduls from this project.  We manually set the\n",
    "# PYTHONPATH to append the location to search for this assignments\n",
    "# functions to just ensure the imports are found\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# assignment function imports for doctests and github autograding\n",
    "# these are required for assignment autograding\n",
    "from assg_utils import run_unittests, run_doctests\n",
    "from assg_tasks import softmax\n",
    "from assg_tasks import sigmoid\n",
    "#from assg_tasks import rnn_cell_forward\n",
    "#from assg_tasks import rnn_forward\n",
    "#from assg_tasks import lstm_cell_forward\n",
    "#from assg_tasks import lstm_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Forward Propagation of a basic Recurrent NN Layer\n",
    "\n",
    "The following figure is modified from our text showing a recurrent layer\n",
    "cell unrolled.  I changed notation a bit from our text, to be more\n",
    "precise in the following discussions.\n",
    "\n",
    "![Basic RNN model](../figures/ch10-7-simple-rnn-unrolled.png)\n",
    "\n",
    "**Figure 1**: Basic RNN model unrolled in time.  Shows conceptually the inputs and outputs\n",
    "that occur at timestep $t$.\n",
    "\n",
    "Also in this assignment, we will be getting a detailed look at what the RNN\n",
    "cell computes.  We will use $x^{\\langle t \\rangle}$ to denote the input\n",
    "from a sequence (like a token) at time $t$.\n",
    "\n",
    "If you look closely and compare to our class materials, \n",
    "you will see that we are computing two outputs from an RNN cell computation\n",
    "at time $t$\n",
    "\n",
    "1. What we will call the activation at time $t$: $a^{\\langle t \\rangle}$.\n",
    "2. And what we will call the prediction at time  $t$:\n",
    "   $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1: Implement `rnn_cell_forward` Call\n",
    "\n",
    "The following computations of a single RNN cycle / cell has a few extra details not mentioned\n",
    "in our class materials.  It is a bit closer to the original description of the RNN layer,\n",
    "where two outputs, the activation (used as the RNN state) and a prediction (e.g. the actual predicted\n",
    "output target at time $t$, are produced by the `rnn_cell_forward()`.\n",
    "\n",
    "A recurrent neural network can be seen as the repeated use of a single cell or computation.  You are\n",
    "going to first implement the computations for a single time-step.  The following figure describes in\n",
    "more detail the tensor operations for a single time-step of an RNN cell.\n",
    "\n",
    "![Basic RNN cell](../figures/rnn-cell.png)\n",
    "\n",
    "**Figure 2**: Basic RNN cell / computations at time $t$.  Cell takes an input $x^{\\langle t \\rangle}$\n",
    "(current input) and $a^{\\langle t-1 \\rangle}$ (previous hidden state activation containing information\n",
    "from the past), and outputs $a^{\\langle t \\rangle}$ (hidden state activation for current timestep)\n",
    "which is given to the next RNN cell computation, and used to make prediction $\\hat{y}^{\\langle t \\rangle}$\n",
    "for this timestep 2.\n",
    "\n",
    "**Task**: Implement the `rnn_cell_forward()` method.  \n",
    "\n",
    "The size of the inputs, the internal state activations, and the outputs can all differ.  For\n",
    "example, in our weather prediction model there were 14 weather measurements that were recorded\n",
    "at each 10 minute interval. Call the size of the input features `n_x`.\n",
    "\n",
    "Likewise we can specify the number of outputs a recurrent layer should create and learn.\n",
    "This doesn't have to be equal to the numper of input features.  Call this `n_y`.\n",
    "\n",
    "But also, in the original specification of the RNN layer, the size of the internel state\n",
    "activations didn't necessarily have to be either `n_x` nor `n_y`.  Call the size of the\n",
    "internal state activations `n_a`.\n",
    "\n",
    "This method will be given the following input parameters:\n",
    "\n",
    "- The current input  $x^{\\langle t \\rangle}$.  This will be a vector equal to the number of\n",
    "  features in a sample, for example in our weather prediction example there were\n",
    "  14 weather measurements each time step, so `n_x = 14` and the input would be\n",
    "  vectors of shape `(14,)`\n",
    "- The previous activation state $a^{\\langle t-1 \\rangle}$, call this parameter something\n",
    "  like `a_prev`. This will be a vector of shape `(n_a, )`\n",
    "- There are 3 weight matrices and two bias matrices:\n",
    "  - $W_a$ will be multiplied by the inputs, so it has a shape `(n_x, n_a)`\n",
    "  - $U_a$ is the weight matrix multiplied by the activations, so it has a shape `(n_a, n_a)`\n",
    "  - $b_a$ will be a vector of shape `(n_a,)`\n",
    "  - $W_y$ is multiplied by the state activations, so its shape is `(n_a, n_y)`\n",
    "  - $b_y$ will be a vector of shape `(n_y,)`\n",
    "  - All of these parameters are passed in as a tuple for the third parameter in this order\n",
    "    `(W_a, U_a, b_a, W_y, b_y)`\n",
    "\n",
    "And as you may be able to guess, the circle with `x` represent matrix multiplications (dot product),\n",
    "and the circle with `+` are adds of the biases.\n",
    "\n",
    "So your forward cell will be calculating the following functions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "a^{\\langle t \\rangle} &= \\tanh(x^{\\langle t \\rangle} W_a + a^{\\langle t-1 \\rangle} U_a + b_a) \\\\\n",
    "\\hat{y}^{\\langle t \\rangle} &= \\text{softmax}(a^{\\langle t \\rangle} W_y + b_y) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The cell calculations should be vectorized.  And actually the input $x^{\\langle t \\rangle}$ is not just a \n",
    "simple vector, we will be passing in batches.  So for example lets say the batch size is 64 (call this `batch_size`), and\n",
    "the number of features in the input is `n_x = 14`.  Then the actual of shape of $x^{\\langle t \\rangle}$\n",
    "will be `(batch_size, nx)` = `(64, 14)`.  That means that the actual activations $a^{\\langle t \\rangle}$ are of shape `(batch_size, n_a)`,\n",
    "and the predicted outputs $\\hat{y}^{\\langle t \\rangle}$ will be of shape `(batch_size, n_y)`.\n",
    "\n",
    "Your function should take the described 3 inputs and output the calculated $a^{\\langle t \\rangle}$\n",
    "and $\\hat{y}^{\\langle t \\rangle}$ for the given batch of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t:  (10, 3)\n",
      "a_prev:  (10, 5)\n",
      "W_a:  (3, 5)\n",
      "U_a:  (5, 5)\n",
      "b_a:  (5,)\n",
      "W_y:  (5, 2)\n",
      "b_y:  (2,)\n",
      "a_t:  (2, 2)\n",
      "y_t:  (2, 2)\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "### TESTED function rnn_cell_forward()\n",
    "# uncomment when ready to run the unit tests for function\n",
    "#run_unittests(['test_rnn_cell_forward'])\n",
    "\n",
    "# a test to check performing cell tensor operations as expected\n",
    "# example where we have a batch size of 10, number of input features is 3, number of\n",
    "# output features is 2, and the internal state carries 5 activations\n",
    "batch_size = 10\n",
    "n_x = 3\n",
    "n_a = 5\n",
    "n_y = 2\n",
    "\n",
    "# create some random matrices with a set seed so we always get same output result\n",
    "np.random.seed(1)\n",
    "x_t = np.random.randn(batch_size, n_x)\n",
    "a_prev = np.random.randn(batch_size, n_a)\n",
    "\n",
    "W_a = np.random.randn(n_x, n_a)\n",
    "U_a = np.random.randn(n_a, n_a)\n",
    "b_a = np.random.randn(n_a)\n",
    "W_y = np.random.randn(n_a, n_y)\n",
    "b_y = np.random.randn(n_y)\n",
    "\n",
    "print('x_t: ', x_t.shape)\n",
    "print('a_prev: ', a_prev.shape)\n",
    "print('W_a: ', W_a.shape)\n",
    "print('U_a: ', U_a.shape)\n",
    "print('b_a: ', b_a.shape)\n",
    "print('W_y: ', W_y.shape)\n",
    "print('b_y: ', b_y.shape)\n",
    "\n",
    "a_t = np.zeros((2,2))\n",
    "y_t = np.zeros((2,2))\n",
    "#a_t, y_t = rnn_cell_forward(x_t, a_prev, (W_a, U_a, b_a, W_y, b_y))\n",
    "\n",
    "print('a_t: ', a_t.shape)\n",
    "print('y_t: ', y_t.shape)\n",
    "\n",
    "# 3rd and 4th sample of a and y\n",
    "print(a_t[3:5])\n",
    "print(y_t[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results** Given the random seed, you should get the following results if your calculations in `rnn_cell_forward()` are performed correctly.\n",
    "\n",
    "```\n",
    "x_t:  (10, 3)\n",
    "a_prev:  (10, 5)\n",
    "W_a:  (3, 5)\n",
    "U_a:  (5, 5)\n",
    "b_a:  (5,)\n",
    "W_y:  (5, 2)\n",
    "b_y:  (2,)\n",
    "a_t:  (10, 5)\n",
    "y_t:  (10, 2)\n",
    "[[-0.99523806 -0.06488166 -0.28849778  0.99917567 -0.88942078]\n",
    " [ 0.97054882 -0.93853384  0.92825359  0.77745675 -0.90964872]]\n",
    "[[0.42204273 0.57795727]\n",
    " [0.21579613 0.78420387]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: RNN Forward Pass\n",
    "\n",
    "A recurrent neural network (RNN) layer is a repetition of the RNN cell forward unit \n",
    "you've just built.  If your input sequence data is using a sequence length of 10,\n",
    "then you will need use a loop that iterates the `sequence_length` number of times, calling\n",
    "your `rnn_cell_forward()` function to calculate the outputs and activations states for each\n",
    "input of the sequence.\n",
    "\n",
    "[]()\n",
    "\n",
    "**Task**: Code the forward propagation of a full RNN layer sequence as a function named `rnn_forward()`.\n",
    "Some definitions of values we use in the description here.  \n",
    "\n",
    "- `batch_size`: The size of a batch to process\n",
    "- `sequence_length`: The number of time stepse in the sequence/time series input data\n",
    "- `n_x`, `n_a`, `n_y`: As before, the number of features of the input, the number of activation state values,\n",
    "  and the number of features output for each item in the sequence.\n",
    "\n",
    "Your function will take the following parameters:\n",
    "\n",
    "- `x`: A batch of inputs.  The inputs for the whole RNN layer are of shape `(batch_size, sequence_length, n_x)`.\n",
    "  That is to say, to compute the forward pass, the layer is given number of samples as a batch.  All samples\n",
    "  in the batch are sequences of length `sequence_length`.  For example, for our weather forcasting timeseries\n",
    "  task, we had 5 days of samples taken once an hour, so the sequence length there was $5 \\times 24 = 120$.\n",
    "  And at a single measurement time there are `n_x` features that were measured and are given as input, for\n",
    "  example there were 14 weather measurements in our weather prediction task.\n",
    "- `a_init`: The initial value of the stat activations before iterating.  Could be 0, but sometimes we need to\n",
    "  set the initial state (e.g. remember encoder/decoder architecture).  Recall the activations are of\n",
    "  shape `(batch_size, n_a)`.\n",
    "- `parameters`: All of the same parameters are again passed into this function, to be passed along\n",
    "   and used by your `rnn_cell_forward()`.  They are passed in as a tuple for the third parameter in this order\n",
    "    `(W_a, U_a, b_a, W_y, b_y)`\n",
    "\n",
    "This function will need to iterate over all `sequence_length` sequence items, pulling out the batch of items\n",
    "for time `t`.  This function returns the activations and output predictions.  However these will now both\n",
    "be 3D tensors.  The predicted outputs $\\hat{y}$ will be of shape `(batch_size, sequence_length, n_y)`\n",
    "and the hidden activation states will be of shape `(batch_size, sequence_length, n_a)`.\n",
    "\n",
    "This function will be some more practice with manipulating numpy arrays, but hopefully you have the idea here.\n",
    "The pseudo-code for this function is:\n",
    "\n",
    "1. Extract the `batch_size` and `sequence_length` from your inputs (it is dimension 1).\n",
    "   You also need the `n_a` and `n_y`, which is available in multiple ways from the shape of your `parameters`\n",
    "2. Initialize an activations and y predictions array of the needed shape.  You can for example initialize these\n",
    "   to 0.\n",
    "3. Initialize your current activations to the initial activations you were given `a_init`.\n",
    "4. Iterate t = `0` .. `sequence_length`\n",
    "   - Extract the inputs for time `t`.  You need all batches and all input features for time `t`.  Recall that your\n",
    "     `rnn_call_forward()` takes 2D tensors as input.\n",
    "   - Call your `rnn_cell_forward()` with the slice out inputs, the current state activation, and the parameters.\n",
    "   - Your function returns the new calculated activations and y prediction outputs.  Save those in your 3D arrays\n",
    "     you will return of all the activations and predictions\n",
    "   - Make sure that you have updated the activations so the new ones are passed into the cell forward call\n",
    "     in next iteration.\n",
    "\n",
    "If you perform the iteration correctly, you should have the history of activations and predicted outputs\n",
    "as 3D tensors, that should be returned from this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (10, 8, 3)\n",
      "a_init:  (10, 5)\n",
      "W_a:  (3, 5)\n",
      "U_a:  (5, 5)\n",
      "b_a:  (5,)\n",
      "W_y:  (5, 2)\n",
      "b_y:  (2,)\n",
      "a:  (10, 2)\n",
      "y:  (10, 2)\n",
      "[0. 0.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "### TESTED function rnn_forward()\n",
    "# uncomment when ready to run the unit tests for function\n",
    "#run_unittests(['test_rnn_forward'])\n",
    "\n",
    "# a test to check performing cell tensor operations as expected\n",
    "# example where we have a batch size of 10, number of input features is 3, number of\n",
    "# output features is 2, and the internal state carries 5 activations\n",
    "batch_size = 10\n",
    "sequence_length = 8\n",
    "n_x = 3\n",
    "n_a = 5\n",
    "n_y = 2\n",
    "\n",
    "# create some random matrices with a set seed so we always get same output result\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(batch_size, sequence_length, n_x)\n",
    "a_init = np.random.randn(batch_size, n_a)\n",
    "\n",
    "W_a = np.random.randn(n_x, n_a)\n",
    "U_a = np.random.randn(n_a, n_a)\n",
    "b_a = np.random.randn(n_a)\n",
    "W_y = np.random.randn(n_a, n_y)\n",
    "b_y = np.random.randn(n_y)\n",
    "\n",
    "print('x: ', x.shape)\n",
    "print('a_init: ', a_init.shape)\n",
    "print('W_a: ', W_a.shape)\n",
    "print('U_a: ', U_a.shape)\n",
    "print('b_a: ', b_a.shape)\n",
    "print('W_y: ', W_y.shape)\n",
    "print('b_y: ', b_y.shape)\n",
    "\n",
    "a = np.zeros((10,2))\n",
    "y = np.zeros((10,2))\n",
    "#a, y = rnn_forward(x, a_init, (W_a, U_a, b_a, W_y, b_y))\n",
    "\n",
    "print('a: ', a.shape)\n",
    "print('y: ', y.shape)\n",
    "\n",
    "# the 6th sample in batch of each\n",
    "print(a[6])\n",
    "print(y[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results**: For the given seed, we expect you to get the following output from your `rnn_forward()`\n",
    "function if everything is implemented correctly:\n",
    "\n",
    "```\n",
    "x:  (10, 8, 3)\n",
    "a_init:  (10, 5)\n",
    "W_a:  (3, 5)\n",
    "U_a:  (5, 5)\n",
    "b_a:  (5,)\n",
    "W_y:  (5, 2)\n",
    "b_y:  (2,)\n",
    "a:  (10, 8, 5)\n",
    "y:  (10, 8, 2)\n",
    "[[ 0.99678874  0.95856544  0.03469727 -0.97763179  0.99976266]\n",
    " [ 0.99996493  0.52446059  0.44327442 -0.67532026  0.99952257]\n",
    " [ 0.99999831 -0.80330671 -0.99409029  0.94567413  0.80104963]\n",
    " [ 0.22600828 -0.70257738 -0.02746938 -0.77244757  0.71340253]\n",
    " [ 0.70489704  0.73414165 -0.9316695   0.8268125  -0.46866482]\n",
    " [-0.5500474   0.93933467 -0.70401339  0.96963972  0.99945562]\n",
    " [ 0.95698489 -0.88623095  0.93160166 -0.95792092  0.99999674]\n",
    " [ 0.99534743  0.92742305 -0.90651793 -0.05962163 -0.90932672]]\n",
    "[[0.69506766 0.30493234]\n",
    " [0.6006428  0.3993572 ]\n",
    " [0.99191778 0.00808222]\n",
    " [0.29487726 0.70512274]\n",
    " [0.98783339 0.01216661]\n",
    " [0.96180779 0.03819221]\n",
    " [0.14607254 0.85392746]\n",
    " [0.95925418 0.04074582]]\n",
    "```\n",
    "\n",
    "Congratulations! You've successfully built the forward propagation of a recurrent neural network\n",
    "from scratch.\n",
    "\n",
    "**Situations when this RNN will perform better**:\n",
    "\n",
    "- This will work well enough for some applications, but it suffers from the vanishing gradient problems. \n",
    "- The RNN works best when each output $\\hat{y}^{\\langle t \\rangle}$ can be estimated using \"local\" context.  \n",
    "- \"Local\" context refers to information that is close to the prediction's time step $t$.\n",
    "- More formally, local context refers to inputs $x^{\\langle t' \\rangle}$ and predictions $\\hat{y}^{\\langle t \\rangle}$ where $t'$ is close to $t$.\n",
    "\n",
    "In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Long Short Term Memory (LSTM) network\n",
    "\n",
    "## Task 2.1: LSTM cell forward\n",
    "The LSTM cell is a bit more complicated.  The following shows the operation of an LSTM-cell:\n",
    "\n",
    "![LSTM Cell](../figures/lstm-cell.png)\n",
    "\n",
    "**Figure 3**: LSTM-cell. This tracks and updates a \"memory cell state\" or memory variable $m^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$\n",
    "The 4 gates shown, forget, update, memory and output, all involve a basic tensor multiplication and add of bias.  Thier function is to update\n",
    "the memory cell state in different ways (in theory).\n",
    "\n",
    "This task will be similar to the previous one, you will first implement a function to perform\n",
    "the tensor operations for a single timestep.  Then a function with an iterative loop that will call\n",
    "the first function the needed number of times to calculate the forward pass.\n",
    "\n",
    "**Task**: Implement the `lstm_cell_forward()` method.  \n",
    "\n",
    "Like before, the size of the inputs, internal state activations and outputs can all differ.\n",
    "For the LSTM layer there is an internel memory channel in addition to the internal state activations.\n",
    "We will call the inputs, activations, memory and y predictions $x, a, m, y$ respectively. The\n",
    "internal memory channel must have the same number of units as the state activations, so `n_m = n_a`\n",
    "but otherwise `n_x` and `n_y` can be different from `n_m, n_a`.\n",
    "\n",
    "This metod will be given the following input parameters:\n",
    "\n",
    "- The current input  $x^{\\langle t \\rangle}$.  This will be a vector equal to the number of\n",
    "  features in a sample, for example in our weather prediction example there were\n",
    "  14 weather measurements each time step, so `n_x = 14` and the input would be\n",
    "  vectors of shape `(14,)`\n",
    "- The previous activation state $a^{\\langle t-1 \\rangle}$, call this parameter something\n",
    "  like `a_prev`. This will be a vector of shape `(n_a, )`\n",
    "- The previous memory cell state $m^{\\langle t-1 \\rangle}$, call this parameter something\n",
    "  like `m_prev`.  The number of units must be the same as the state activations so `n_m = n_a`.\n",
    "- There are forget, update, memory, output and y predictions operations in the LSTM cell, all need\n",
    "  a weight matrix and bias vector for their calculations:\n",
    "  - $W_f, b_f$ are the tensors for the forget gate with shape `(n_a + n_x, n_a)` and `(n_a,)`\n",
    "  - $W_u, b_u$ are the tensors for the update gate with shape `(n_a + n_x, n_a)` and `(n_a,)`\n",
    "  - $W_m, b_m$ are the tensors for the memory gate with shape `(n_a + n_x, n_a)` and `(n_a,)`\n",
    "  - $W_o, b_o$ are the tensors for the output (activations) with shape `(n_a + n_x, n_a)` and `(n_a,)`\n",
    "  - $W_y, b_y$ are the tensors for the y predictions with shape `(n_a, n_y)` and `(n_y, `)\n",
    "  - All of these parameters are passed in as a tuple for the fourth parameter in this order\n",
    "    `(W_f, b_f, W_u, b_u, W_m, b_m, W_o, b_o, W_y, b_y)`\n",
    "\n",
    "The operations using the `forget`, `memory` and `update` gates are a bit more complex than before, but\n",
    "mathematically can be expressed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "C = [ a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle} ]\n",
    "\\end{equation}\n",
    "\n",
    "We will use the concatenation of the previous activations `a_prev` and the current inputs `x_t` in the\n",
    "following expressions (Hint: np.concatenate()).  For example if the batch size is 10 and the\n",
    "previous activations are of shape `(10, 5)` and the current inputs are of shape `(10, 3)`,\n",
    "then concatenating `a_prev` and `x_t` results in a matrix with still 10 rows, but now 8 columns.\n",
    "Be sure that the previous activations end up in columns 0-4 and the inputs in columns 5-7 when\n",
    "you concatenate, or you won't get the correct results.\n",
    "\n",
    "Given the concatenation `D` of the previous activations and the current inputs, the `forget`,\n",
    "`update`, `memory` and `output` gate operations can be defined simply as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\text{forget} &= \\text{sigmoid}( C W_f + b_f) \\\\\n",
    "\\text{update} &= \\text{sigmoid}( C W_u + b_u) \\\\\n",
    "\\text{memory} &= \\text{tanh}( C W_m + b_m) \\\\\n",
    "\\text{output} &= \\text{sigmoid}( C W_o + b_o) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "As you can see these are all similar and all use the basic tensor operations of a matrix multiplication and addition you are familiar with.  The `memory` gate uses a `tanh` (see `np.tanh()`) activation function while the result use the `sigmoid`.  You can find `sigmoid` and `softmax` functions in TensorFlow, but they will return Tensor objects, and we want to stick with straight numpy in this\n",
    "assignment and not import the `tensorflow` library.  You implemented `sigmoid` and `softmax` before,\n",
    "so we have given implementations of these fucntions for you to use in your `assg_tasks.py` file.\n",
    "\n",
    "The get outputs are then used to calculate the resulting `a_t` (activation state at time t),\n",
    "`m_t` (memory cell state at time t), and `y_t` (y predictions at time t).  These are the 3 results\n",
    "that are returned from this function `(a_t, m_t, y_t)`.  These three final outputs are calculated\n",
    "using the gates like this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "m_t &= \\text{forget} \\times m^{\\langle t-1 \\rangle} + \\text{update} \\times \\text{memory} \\\\\n",
    "a_t &= \\text{output} \\times \\tanh(m_t) \\\\\n",
    "y_t &= \\text{softmax}( a_t W_y + b_y ) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The $\\times$ are indicating element wise multiplications rather than matrix multiplications here.\n",
    "The previous two equations represent the full update function of the LSTM recurrent layer.  The \n",
    "forget, memory, update and output weights are separate sets of tensor matrixs that can be learned.\n",
    "As you can see the design is highly engineered.  The names of the gates give some sense of the\n",
    "purpose intended for them, though it is questionable if they really work as they were originally\n",
    "intended.  But the memory state is basically updated using an elementwise multiplcation and\n",
    "addition.  This ends up being a linear operation, and the update to $m^{\\langle t \\rangle}$ ends\n",
    "up not changing too much and in a linear fashin.  So the memory state acts as a kind of skip\n",
    "connection (residual connection) in LSTM cell iterations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t:  (10, 3)\n",
      "a_prev:  (10, 5)\n",
      "m_prev:  (10, 5)\n",
      "W_f:  (8, 5)\n",
      "b_f:  (5,)\n",
      "W_u:  (8, 5)\n",
      "b_u:  (5,)\n",
      "W_m:  (8, 5)\n",
      "b_m:  (5,)\n",
      "W_o:  (8, 5)\n",
      "b_o:  (5,)\n",
      "W_y:  (5, 2)\n",
      "b_y:  (2,)\n",
      "a_t:  (2, 2)\n",
      "m_t:  (2, 2)\n",
      "y_t:  (2, 2)\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "### TESTED function lstm_cell_forward()\n",
    "# uncomment when ready to run the unit tests for function\n",
    "#run_unittests(['test_lstm_cell_forward'])\n",
    "\n",
    "# a test to check performing cell tensor operations as expected\n",
    "# example where we have a batch size of 10, number of input features is 3, number of\n",
    "# output features is 2, and the internal state carries 5 activations\n",
    "batch_size = 10\n",
    "n_x = 3\n",
    "n_a = 5\n",
    "n_y = 2\n",
    "\n",
    "# create some random matrices with a set seed so we always get same output result\n",
    "np.random.seed(1)\n",
    "x_t = np.random.randn(batch_size, n_x)\n",
    "a_prev = np.random.randn(batch_size, n_a)\n",
    "m_prev = np.random.randn(batch_size, n_a)\n",
    "\n",
    "W_f = np.random.randn(n_a + n_x, n_a)\n",
    "b_f = np.random.randn(n_a)\n",
    "W_u = np.random.randn(n_a + n_x, n_a)\n",
    "b_u = np.random.randn(n_a)\n",
    "W_m = np.random.randn(n_a + n_x, n_a)\n",
    "b_m = np.random.randn(n_a)\n",
    "W_o = np.random.randn(n_a + n_x, n_a)\n",
    "b_o = np.random.randn(n_a)\n",
    "W_y = np.random.randn(n_a, n_y)\n",
    "b_y = np.random.randn(n_y)\n",
    "\n",
    "print('x_t: ', x_t.shape)\n",
    "print('a_prev: ', a_prev.shape)\n",
    "print('m_prev: ', m_prev.shape)\n",
    "print('W_f: ', W_f.shape)\n",
    "print('b_f: ', b_f.shape)\n",
    "print('W_u: ', W_u.shape)\n",
    "print('b_u: ', b_u.shape)\n",
    "print('W_m: ', W_m.shape)\n",
    "print('b_m: ', b_m.shape)\n",
    "print('W_o: ', W_o.shape)\n",
    "print('b_o: ', b_o.shape)\n",
    "print('W_y: ', W_y.shape)\n",
    "print('b_y: ', b_y.shape)\n",
    "\n",
    "a_t = np.zeros((2,2))\n",
    "m_t = np.zeros((2,2))\n",
    "y_t = np.zeros((2,2))\n",
    "#a_t, m_t, y_t = lstm_cell_forward(x_t, a_prev, m_prev, (W_f, b_f, W_u, b_u, W_m, b_m, W_o, b_o, W_y, b_y))\n",
    "\n",
    "print('a_t: ', a_t.shape)\n",
    "print('m_t: ', m_t.shape)\n",
    "print('y_t: ', y_t.shape)\n",
    "\n",
    "# 3rd and 4th sample of a_t, m_t and y_t\n",
    "print(a_t[3:5])\n",
    "print(m_t[3:5])\n",
    "print(y_t[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Forward pass for LSTM\n",
    "\n",
    "The hard works for implementing the LSTM forward pass has mostly been done, and your\n",
    "implementation of the full forward pass for the LSTM layer will be similar to what\n",
    "you previously did for the simple RNN.\n",
    "\n",
    "**Task**: Code the forward propagation of a full LSTM layer sequence as a function named `lstm_forward()`.\n",
    "Some definitions of values we use in the description here.  \n",
    "\n",
    "- `batch_size`: The size of a batch to process\n",
    "- `sequence_length`: The number of time stepse in the sequence/time series input data\n",
    "- `n_x`, `n_a`, `n_y`: As before, the number of features of the input, the number of activation state values,\n",
    "  and the number of features output for each item in the sequence.  The number of features for the\n",
    "  memory state `n_m` will be equal to the number of activations `n_a`.\n",
    "\n",
    "Your function will take the following parameters:\n",
    "\n",
    "- `x`: A batch of inputs.  The inputs for the whole LSTM layer are of shape `(batch_size, sequence_length, n_x)`.\n",
    "  That is to say, to compute the forward pass, the layer is given number of samples as a batch.  All samples\n",
    "  in the batch are sequences of length `sequence_length`.  For example, for our weather forcasting timeseries\n",
    "  task, we had 5 days of samples taken once an hour, so the sequence length there was $5 \\times 24 = 120$.\n",
    "  And at a single measurement time there are `n_x` features that were measured and are given as input, for\n",
    "  example there were 14 weather measurements in our weather prediction task.\n",
    "- `a_init`: The initial value of the stat activations before iterating.  Could be 0, but sometimes we need to\n",
    "  set the initial state (e.g. remember encoder/decoder architecture).  Recall the activations are of\n",
    "  shape `(batch_size, n_a)`.\n",
    "- `parameters`: All of the same parameters are again passed into this function, to be passed along\n",
    "   and used by your `lstm_cell_forward()`.  They are passed in as a tuple for the third parameter in this order\n",
    "    `(W_f, b_f, W_u, b_u, W_m, b_m, W_o, b_o, W_y, b_y)`\n",
    "\n",
    "An observant student might have been expecting an `m_init` to initialize the memory state, the same\n",
    "as the `a_init`.  However we need to be able to initialize the activations at the start of the\n",
    "forward state.  But the cell memory state always starts out as 0, so `m_init` will be needed but you will initialize it to 0 before beginning your time step iterations.\n",
    "\n",
    "\n",
    "This function will need to iterate over all `sequence_length` sequence items, pulling out the batch of items\n",
    "for time `t`.  This function returns the activation, memory state and output predictions that result\n",
    "from all of the iterations.  However these will now\n",
    "be 3D tensors.  The predicted outputs $\\hat{y}$ will be of shape `(batch_size, sequence_length, n_y)`\n",
    "and the hidden activation states and memory cell states will both be shaped `(batch_size, sequence_length, n_a)`.\n",
    "\n",
    "The pseudo-code for this function is:\n",
    "\n",
    "1. Extract the `batch_size` and `sequence_length` from your inputs (it is dimension 1).\n",
    "   You also need the `n_a` and `n_y`, which is available in multiple ways from the shape of your `parameters`\n",
    "2. Initialize an activations and y predictions array of the needed shape.  You can for example initialize these\n",
    "   to 0.\n",
    "3. Initialize your current activations to the initial activations you were given `a_init`.\n",
    "4. Iterate t = `0` .. `sequence_length`\n",
    "   - Extract the inputs for time `t`.  You need all batches and all input features for time `t`.  Recall that your\n",
    "     `rnn_call_forward()` takes 2D tensors as input.\n",
    "   - Call your `rnn_cell_forward()` with the slice out inputs, the current state activation, and the parameters.\n",
    "   - Your function returns the new calculated activations and y prediction outputs.  Save those in your 3D arrays\n",
    "     you will return of all the activations and predictions\n",
    "   - Make sure that you have updated the activations so the new ones are passed into the cell forward call\n",
    "     in next iteration.\n",
    "\n",
    "If you perform the iteration correctly, you should have the history of activations and predicted outputs\n",
    "as 3D tensors, that should be returned from this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (10, 8, 3)\n",
      "a_init:  (10, 5)\n",
      "W_f:  (8, 5)\n",
      "b_f:  (5,)\n",
      "W_u:  (8, 5)\n",
      "b_u:  (5,)\n",
      "W_m:  (8, 5)\n",
      "b_m:  (5,)\n",
      "W_o:  (8, 5)\n",
      "b_o:  (5,)\n",
      "W_y:  (5, 2)\n",
      "b_y:  (2,)\n",
      "a:  (10, 2)\n",
      "m:  (10, 2)\n",
      "y:  (10, 2)\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "### TESTED function lstm_forward()\n",
    "# uncomment when ready to run the unit tests for function\n",
    "#run_unittests(['test_lstm_forward'])\n",
    "\n",
    "# a test to check performing cell tensor operations as expected\n",
    "# example where we have a batch size of 10, number of input features is 3, number of\n",
    "# output features is 2, and the internal state carries 5 activations\n",
    "batch_size = 10\n",
    "sequence_length = 8\n",
    "n_x = 3\n",
    "n_a = 5\n",
    "n_y = 2\n",
    "\n",
    "# create some random matrices with a set seed so we always get same output result\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(batch_size, sequence_length, n_x)\n",
    "a_init = np.random.randn(batch_size, n_a)\n",
    "\n",
    "W_f = np.random.randn(n_a + n_x, n_a)\n",
    "b_f = np.random.randn(n_a)\n",
    "W_u = np.random.randn(n_a + n_x, n_a)\n",
    "b_u = np.random.randn(n_a)\n",
    "W_m = np.random.randn(n_a + n_x, n_a)\n",
    "b_m = np.random.randn(n_a)\n",
    "W_o = np.random.randn(n_a + n_x, n_a)\n",
    "b_o = np.random.randn(n_a)\n",
    "W_y = np.random.randn(n_a, n_y)\n",
    "b_y = np.random.randn(n_y)\n",
    "\n",
    "print('x: ', x.shape)\n",
    "print('a_init: ', a_init.shape)\n",
    "print('W_f: ', W_f.shape)\n",
    "print('b_f: ', b_f.shape)\n",
    "print('W_u: ', W_f.shape)\n",
    "print('b_u: ', b_f.shape)\n",
    "print('W_m: ', W_f.shape)\n",
    "print('b_m: ', b_f.shape)\n",
    "print('W_o: ', W_f.shape)\n",
    "print('b_o: ', b_f.shape)\n",
    "print('W_y: ', W_y.shape)\n",
    "print('b_y: ', b_y.shape)\n",
    "\n",
    "a = np.zeros((10,2))\n",
    "m = np.zeros((10,2))\n",
    "y = np.zeros((10,2))\n",
    "#a, m, y = lstm_forward(x, a_init, (W_f, b_f, W_u, b_u, W_m, b_m, W_o, b_o, W_y, b_y))\n",
    "\n",
    "print('a: ', a.shape)\n",
    "print('m: ', a.shape)\n",
    "print('y: ', y.shape)\n",
    "\n",
    "# the 6th sample in batch of each\n",
    "print(a[6])\n",
    "print(m[6])\n",
    "print(y[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results**: For the given seeds, if you have done all calculations as specified, you should get the following outputs:\n",
    "\n",
    "```\n",
    "x:  (10, 8, 3)\n",
    "a_init:  (10, 5)\n",
    "W_f:  (8, 5)\n",
    "b_f:  (5,)\n",
    "W_u:  (8, 5)\n",
    "b_u:  (5,)\n",
    "W_m:  (8, 5)\n",
    "b_m:  (5,)\n",
    "W_o:  (8, 5)\n",
    "b_o:  (5,)\n",
    "W_y:  (5, 2)\n",
    "b_y:  (2,)\n",
    "a:  (10, 8, 5)\n",
    "m:  (10, 8, 5)\n",
    "y:  (10, 8, 2)\n",
    "[[ 0.33106747  0.13796993 -0.00918376 -0.14513632  0.0842039 ]\n",
    " [-0.02811128  0.28693576 -0.02530277 -0.036286    0.2427269 ]\n",
    " [-0.24675213 -0.1234593  -0.61121026 -0.05457205  0.66088247]\n",
    " [-0.05236292 -0.70535923 -0.47803672  0.03029571  0.77776549]\n",
    " [-0.16682891 -0.08195929 -0.07687751  0.1187123   0.24526771]\n",
    " [ 0.36712994  0.24792886 -0.22405346  0.02844528  0.22654813]\n",
    " [ 0.04708272  0.06898404 -0.512682   -0.00995657  0.48537134]\n",
    " [-0.14107302  0.15767014 -0.33808058 -0.03324728  0.18678606]]\n",
    "[[ 0.43974553  0.1625789  -0.00975767 -0.15896916  0.11043907]\n",
    " [-0.08665783  0.51434267 -0.02604385 -0.05949502  0.4851059 ]\n",
    " [-0.27797833 -0.16043439 -1.01003334 -0.05720113  1.30363934]\n",
    " [-0.3050144  -1.14317762 -0.5422558   0.22876783  1.86032122]\n",
    " [-0.45851336 -0.41038619 -0.09937711  0.19449822  2.52408118]\n",
    " [ 0.50119446  0.49577678 -0.68611907  0.03542264  2.41137585]\n",
    " [ 0.08260235  0.12430022 -0.62310968 -0.01389028  2.89012875]\n",
    " [-0.44837114  1.01221385 -0.37985992 -0.07050988  2.35030661]]\n",
    "[[0.86029619 0.13970381]\n",
    " [0.78735166 0.21264834]\n",
    " [0.91794355 0.08205645]\n",
    " [0.97988862 0.02011138]\n",
    " [0.86544253 0.13455747]\n",
    " [0.84260115 0.15739885]\n",
    " [0.88703823 0.11296177]\n",
    " [0.80061921 0.19938079]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Backpropagation in Recurrent Neural Networks (Optional / Ungraded)\n",
    "\n",
    "**Todo**: In future add in optional / ungraded walk through of calculating\n",
    "backpropagation by unrolling RNN iterations.\n",
    "\n",
    "In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If however you are an expert in calculus and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook. \n",
    "\n",
    "When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks you can calculate the derivatives with respect to the cost in order to update the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Congratulations on completing this assignment. Hopefully you now have better insights into how\n",
    "recurrent layers compute their outputs given a batch of sequence data.\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**What to remember from this assignment:**\n",
    "\n",
    "- Operations in recurrent layers are done (conceptually) one time step at a time.\n",
    "- The operations are tensor operations that you are familiar with.\n",
    "- For simple RNN, two weight tensors, that are multiplied by the inputs and the previous\n",
    "  activation states are applied and combined with a bias to generate the next activation state\n",
    "  and output from the RNN cell.\n",
    "- LSTM contain an extra memory cell state that is propagated along the time step sequence.\n",
    "- LSTM layers use a (rather overcomplicated) set of preliminary gates, known as forget, memory,\n",
    "  update, and output gates, in the sequence of calculations to produce the final y predictions\n",
    "  and internal activations and memory state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
